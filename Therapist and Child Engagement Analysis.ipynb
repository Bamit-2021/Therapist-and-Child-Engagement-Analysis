{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install torch torchvision\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install Segmentor\n",
    "# !pip install grounded-segment-anything\n",
    "# !pip install gaze_transformer\n",
    "# !pip install opencv-python Pillow torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"C:/Users/Amit/Desktop/Project/ABA Therapy_ Daniel - Communication.mp4\"\n",
    "preprocessed_frames = preprocess_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ac6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_gaze(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor()])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        gaze_output = gaze_transformer(image_tensor)\n",
    "\n",
    "    gaze_x, gaze_y = gaze_output[0], gaze_output[1]\n",
    "    return gaze_x, gaze_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc48ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_object_interaction(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    output = segmentor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e005ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPredictor:\n",
    "    def __call__(self, frame):\n",
    "        return \"Happy\", \"Neutral\"\n",
    "emotion_predictor = EmotionPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(frame, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion):\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.scatter(child_gaze[0] * frame.shape[1], child_gaze[1] * frame.shape[0], c='r', marker='o', label='Child Gaze')\n",
    "    plt.scatter(therapist_gaze[0] * frame.shape[1], therapist_gaze[1] * frame.shape[0], c='b', marker='o', label='Therapist Gaze')\n",
    "\n",
    "    for i, interaction in enumerate(interactions):\n",
    "        plt.text(10, 20+i*20, interaction, color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    plt.text(10, 40+len(interactions)*20, f\"Child Emotion: {child_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    plt.text(10, 60+len(interactions)*20, f\"Therapist Emotion: {therapist_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    plt.savefig('temp_frame.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1443dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path):\n",
    "    frames = preprocess_video(video_path)\n",
    "    results = []\n",
    "    for frame in frames:\n",
    "        child_gaze, therapist_gaze = track_gaze(frame)\n",
    "        interactions = detect_object_interaction(frame)\n",
    "        child_emotion, therapist_emotion = emotion_predictor(frame)\n",
    "        visualize_predictions(frame, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion)\n",
    "        results.append({\n",
    "            \"child_gaze\": child_gaze,\n",
    "            \"therapist_gaze\": therapist_gaze,\n",
    "            \"interactions\": interactions,\n",
    "            \"child_emotion\": child_emotion,\n",
    "            \"therapist_emotion\": therapist_emotion\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f00f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_video(frames, output_video_path):\n",
    "    frames = cv2.imread(frames[0])\n",
    "    height, width, layers = frames.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 25.0, (width, height))\n",
    "    for frame_path in frames:\n",
    "        video.write(cv2.imread(frame_path))\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path):\n",
    "    frames = preprocess_video(video_path)\n",
    "    results = []\n",
    "    for frame in frames:\n",
    "        child_gaze, therapist_gaze = track_gaze(frame)\n",
    "        interactions = detect_object_interaction(frame)\n",
    "        visualize_predictions(frame, child_gaze, therapist_gaze, interactions)\n",
    "        results.append({\n",
    "            \"child_gaze\": child_gaze,\n",
    "            \"therapist_gaze\": therapist_gaze,\n",
    "            \"interactions\": interactions\n",
    "        })\n",
    "    return frames, results\n",
    "\n",
    "video_path = \"C:/Users/Amit/Desktop/Project/ABA Therapy_ Daniel - Communication.mp4\"\n",
    "frames, results = main(video_path)\n",
    "compile_video(frames, \"output_video.mp4\")\n",
    "print(\"Output video saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_predictions(frame, frame_number, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion, output_dir):\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.scatter(child_gaze[0] * frame.shape[1], child_gaze[1] * frame.shape[0], c='r', marker='o', label='Child Gaze')\n",
    "    plt.scatter(therapist_gaze[0] * frame.shape[1], therapist_gaze[1] * frame.shape[0], c='b', marker='o', label='Therapist Gaze')\n",
    "\n",
    "    for i, interaction in enumerate(interactions):\n",
    "        plt.text(10, 20+i*20, interaction, color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    plt.text(10, 40+len(interactions)*20, f\"Child Emotion: {child_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    plt.text(10, 60+len(interactions)*20, f\"Therapist Emotion: {therapist_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"frame_{frame_number}.jpg\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_frame_saving(video_path, output_dir):\n",
    "    frames = preprocess_video(video_path)\n",
    "    results = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        child_gaze, therapist_gaze = track_gaze(frame)\n",
    "        interactions = detect_object_interaction(frame)\n",
    "        child_emotion, therapist_emotion = emotion_predictor(frame)  # Emotion prediction\n",
    "        visualize_and_save_predictions(frame, i, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion, output_dir)\n",
    "        results.append({\n",
    "            \"frame_number\": i,\n",
    "            \"child_gaze\": child_gaze,\n",
    "            \"therapist_gaze\": therapist_gaze,\n",
    "            \"interactions\": interactions,\n",
    "            \"child_emotion\": child_emotion,\n",
    "            \"therapist_emotion\": therapist_emotion\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de142181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile frames into a video\n",
    "def compile_frames_to_video(frames_dir, output_video_path):\n",
    "    frame_files = [os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith('.jpg')]\n",
    "    frame_files.sort()\n",
    "    \n",
    "    frame = cv2.imread(frame_files[0])\n",
    "    height, width, layers = frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 25.0, (width, height))\n",
    "    for frame_file in frame_files:\n",
    "        video.write(cv2.imread(frame_file))\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def compile_frames_to_video(frames_dir, output_video_path):\n",
    "    if not os.path.exists(frames_dir):\n",
    "        print(f\"Output directory '{frames_dir}' does not exist. Creating it...\")\n",
    "        os.makedirs(frames_dir)\n",
    "\n",
    "    frame_files = [os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith('.jpg')]\n",
    "    frame_files.sort()\n",
    "\n",
    "    if not frame_files:\n",
    "        print(\"No frame files found in the output frames directory.\")\n",
    "        return\n",
    "\n",
    "    frame = cv2.imread(frame_files[0])\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 25.0, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        video.write(cv2.imread(frame_file))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "print(\"Output video saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def visualize_and_save_predictions(frame, frame_number, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion, output_dir):\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.scatter(child_gaze[0] * frame.shape[1], child_gaze[1] * frame.shape[0], c='r', marker='o', label='Child Gaze')\n",
    "    plt.scatter(therapist_gaze[0] * frame.shape[1], therapist_gaze[1] * frame.shape[0], c='b', marker='o', label='Therapist Gaze')\n",
    "  \n",
    "    for i, interaction in enumerate(interactions):\n",
    "        plt.text(10, 20+i*20, interaction, color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    \n",
    "    plt.text(10, 40+len(interactions)*20, f\"Child Emotion: {child_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    plt.text(10, 60+len(interactions)*20, f\"Therapist Emotion: {therapist_emotion}\", color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"frame_{frame_number}.jpg\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41317f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_frame_saving(video_path, output_dir):\n",
    "    frames = preprocess_video(video_path)\n",
    "    results = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        child_gaze, therapist_gaze = track_gaze(frame)\n",
    "        interactions = detect_object_interaction(frame)\n",
    "        child_emotion, therapist_emotion = emotion_predictor(frame)  # Emotion prediction\n",
    "        visualize_and_save_predictions(frame, i, child_gaze, therapist_gaze, interactions, child_emotion, therapist_emotion, output_dir)\n",
    "        results.append({\n",
    "            \"frame_number\": i,\n",
    "            \"child_gaze\": child_gaze,\n",
    "            \"therapist_gaze\": therapist_gaze,\n",
    "            \"interactions\": interactions,\n",
    "            \"child_emotion\": child_emotion,\n",
    "            \"therapist_emotion\": therapist_emotion\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e51e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"C:/Users/Amit/Desktop/Project/ABA Therapy_ Daniel - Communication.mp4\"\n",
    "output_dir = \"output_frames\"\n",
    "results = main_with_frame_saving(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import Emotion Predictor, Gaze Transformer, and Object Interaction Detector models\n",
    "from emotion_predictor import EmotionPredictor\n",
    "from gaze_transformer.model import GazePredictionTransformer\n",
    "from object_interaction_detector import ObjectInteractionDetector\n",
    "\n",
    "# Function to preprocess video\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# Function to track gaze using Gaze Transformer model\n",
    "def track_gaze(frame):\n",
    "    # Gaze Transformer model inference\n",
    "    gaze_x, gaze_y = 0.5, 0.5  # Placeholder values\n",
    "    return gaze_x, gaze_y\n",
    "\n",
    "# Function to predict emotions using Emotion Predictor model\n",
    "def predict_emotions(frame):\n",
    "    # Emotion Predictor model inference\n",
    "    child_emotion, therapist_emotion = \"Happy\", \"Neutral\"  # Placeholder values\n",
    "    return child_emotion, therapist_emotion\n",
    "\n",
    "# Function to detect object interactions\n",
    "def detect_object_interactions(frame):\n",
    "    # Object Interaction Detector model inference\n",
    "    interactions = [\"Child playing with toy\", \"Therapist showing object\"]  # Placeholder values\n",
    "    return interactions\n",
    "\n",
    "# Function to visualize predictions and overlay on frame\n",
    "def visualize_and_overlay_predictions(frame, child_gaze, therapist_gaze, child_emotion, therapist_emotion, interactions):\n",
    "    # Overlay predictions on frame\n",
    "    # Code to overlay gaze points, emotions, and interactions on frame\n",
    "\n",
    "    # Display frame with predictions overlaid\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to save frames with predictions overlaid\n",
    "def save_frames_with_predictions(frames, output_dir, child_gazes, therapist_gazes, child_emotions, therapist_emotions, interactions):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        visualize_and_overlay_predictions(frame, child_gazes[i], therapist_gazes[i], child_emotions[i], therapist_emotions[i], interactions[i])\n",
    "        output_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "# Function to compile frames into video\n",
    "def compile_frames_into_video(frames_dir, output_video_path):\n",
    "    frame_files = [os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith('.jpg')]\n",
    "    frame_files.sort()\n",
    "\n",
    "    frame = cv2.imread(frame_files[1000])\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 25.0, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        video.write(cv2.imread(frame_file))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "# Main function\n",
    "def main(video_path, output_dir, output_video_path):\n",
    "    # Step 1: Video Preprocessing\n",
    "    frames = preprocess_video(video_path)\n",
    "\n",
    "    # Step 2: Perform Predictions\n",
    "    child_gazes, therapist_gazes = [], []\n",
    "    child_emotions, therapist_emotions = [], []\n",
    "    interactions = []\n",
    "    for frame in frames:\n",
    "        child_gaze, therapist_gaze = track_gaze(frame)\n",
    "        child_emotion, therapist_emotion = predict_emotions(frame)\n",
    "        interaction = detect_object_interactions(frame)\n",
    "        child_gazes.append(child_gaze)\n",
    "        therapist_gazes.append(therapist_gaze)\n",
    "        child_emotions.append(child_emotion)\n",
    "        therapist_emotions.append(therapist_emotion)\n",
    "        interactions.append(interaction)\n",
    "\n",
    "    # Step 3: Save Frames with Predictions\n",
    "    save_frames_with_predictions(frames, output_dir, child_gazes, therapist_gazes, child_emotions, therapist_emotions, interactions)\n",
    "\n",
    "    # Step 4: Compile Frames into Video\n",
    "    compile_frames_into_video(output_dir, output_video_path)\n",
    "\n",
    "# Example usage\n",
    "video_path = \"example_video.mp4\"\n",
    "output_dir = \"output_frames\"\n",
    "output_video_path = \"output_video.mp4\"\n",
    "main(video_path, output_dir, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae7aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
